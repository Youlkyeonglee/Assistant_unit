{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "train_size:  50000 \n",
      "test_size :  10000\n",
      "('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
      "len class:  10\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# p = Augmentor.Pipeline()\n",
    "# p.random_erasing(0.5,0.4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomCrop((32, 32), padding=4),   #left, top, right, bottom\n",
    "#     p.torch_transform(),\n",
    "#     p.sample(10000),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='/home/lee/Research/11.project_tangqing/data',\n",
    "                               train=True,\n",
    "                               transform=train_transform,\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='/home/lee/Research/11.project_tangqing/data',\n",
    "                              train=False,\n",
    "                              transform=test_transform)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, # 64\n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'test' : test_loader    \n",
    "}\n",
    "image_datasets = {\n",
    "    'train': train_dataset,\n",
    "    'test' : test_dataset\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(image_datasets['train']),\n",
    "    'test' : len(image_datasets['test'])\n",
    "}\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "print('train_size: ',dataset_sizes['train'],'\\ntest_size : ',dataset_sizes['test'])\n",
    "print(classes)\n",
    "print('len class: ', len(classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 class_n_plane\n",
      "1 class_n_car\n",
      "2 class_n_bird\n",
      "3 class_n_cat\n",
      "4 class_n_deer\n",
      "5 class_n_dog\n",
      "6 class_n_frog\n",
      "7 class_n_horse\n",
      "8 class_n_ship\n",
      "9 class_n_truck\n"
     ]
    }
   ],
   "source": [
    "# for n, class_n in enumerate(classes):\n",
    "#     a = 'class_n_{}'.format(class_n)\n",
    "#     print(n, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "# # ADDING NOISE\n",
    "noise = []\n",
    "noise = np.random.rand(32, 32,3)\n",
    "import pickle\n",
    "\n",
    "# pickle_out = open(\"batch1\",\"wb\")\n",
    "data = []\n",
    "filenames = []\n",
    "occludes = []\n",
    "\n",
    "for n in range(5):\n",
    "    inputs, labels = next(iter(dataloaders['train']))\n",
    "#     print(inputs.shape)\n",
    "    pickle_out = open(\"batch{}\".format(n+1),\"wb\")\n",
    "    data = []\n",
    "    count = [0 for i in range(10)]\n",
    "    for m in range(batch_size):\n",
    "        images_ = inputs[m].numpy()\n",
    "        class_  = labels[m].numpy()\n",
    "        images_ = np.transpose(images_, [1,2,0])\n",
    "        for n, class_n in enumerate(classes):\n",
    "            if classes[class_] == class_n:\n",
    "                count[class_] += 1\n",
    "                if count[class_] < 200:\n",
    "                    x_random = np.random.randint(6, 27, size=1, dtype='uint8')\n",
    "                    y_random = np.random.randint(6, 27, size=1, dtype='uint8')\n",
    "                    w_random = np.random.randint(8, 13, size=1, dtype='uint8')\n",
    "                    h_random = np.random.randint(8, 13, size=1, dtype='uint8')\n",
    "                    images_[y_random[0]-int(h_random[0]/2):y_random[0]+int(h_random[0]/2),x_random[0]-int(w_random[0]/2):x_random[0]+int(w_random[0]/2),:]=noise[y_random[0]-int(h_random[0]/2):y_random[0]+int(h_random[0]/2),x_random[0]-int(w_random[0]/2):x_random[0]+int(w_random[0]/2),:]\n",
    "\n",
    "                    r = images_[:,:,0]\n",
    "                    g = images_[:,:,1]\n",
    "                    b = images_[:,:,2]\n",
    "\n",
    "                    images_[:,:,0] = (r - r.min()) / (r.max() - r.min())\n",
    "                    images_[:,:,1] = (g - g.min()) / (g.max() - g.min())\n",
    "                    images_[:,:,2] = (b - b.min()) / (b.max() - b.min())\n",
    "            #             encode_img = images_.tostring()\n",
    "                    data.append(images_)\n",
    "                    filename = '{}_{}.png'.format(classes[class_],class_*10000+m)\n",
    "                    filenames.append(filename)\n",
    "                    occlude = 1\n",
    "                    occludes.append(occlude)\n",
    "\n",
    "#                     plt.imsave('/home/lee/Research/11.project_tangqing/data/cifar-10-occlude/{}_{}.png'.format(classes[class_],class_*10000+m), images_)\n",
    "\n",
    "                else:\n",
    "                    r = images_[:,:,0]\n",
    "                    g = images_[:,:,1]\n",
    "                    b = images_[:,:,2]\n",
    "\n",
    "                    images_[:,:,0] = (r - r.min()) / (r.max() - r.min())\n",
    "                    images_[:,:,1] = (g - g.min()) / (g.max() - g.min())\n",
    "                    images_[:,:,2] = (b - b.min()) / (b.max() - b.min())\n",
    "            #             encode_img = images_.tostring()\n",
    "                    data.append(images_)\n",
    "                    filename = '{}_{}.png'.format(classes[class_],class_*10000+m)\n",
    "                    filenames.append(filename)\n",
    "#         print(classes[class_.numpy()])\n",
    "#                     plt.imsave('/home/lee/Research/11.project_tangqing/data/cifar-10-occlude/{}_{}.png'.format(classes[class_],class_*10000+m), images_)\n",
    "\n",
    "                    occlude = 0\n",
    "                    occludes.append(occlude)\n",
    "#         plt.imshow(images_)\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "    pickle_cifar10 = {'labels': labels.numpy(),'data': data, 'filenames': filenames, 'occludes': occludes}\n",
    "    pickle.dump(pickle_cifar10, pickle_out)\n",
    "    pickle_out.close()\n",
    "    data.clear()\n",
    "    filenames.clear()\n",
    "    count.clear()\n",
    "    occludes.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "#     import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pickle_in = unpickle(\"/home/lee/Research/11.project_tangqing/batch1\")\n",
    "# print(len(pickle_in['occludes']))\n",
    "# print(pickle_in['occludes'])\n",
    "\n",
    "# print(a[100])\n",
    "print(pickle_in['data'][0])\n",
    "print(pickle_in['labels'])\n",
    "print(pickle_in['filenames'])\n",
    "\n",
    "\n",
    "# a = pickle_in['occludes']\n",
    "# count1 = 0\n",
    "# for i in range(10000):\n",
    "#     if pickle_in['labels'][i] == 0:\n",
    "#         if pickle_in['occludes'][i] == 1:\n",
    "#             count1 += 1\n",
    "# print(count1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
